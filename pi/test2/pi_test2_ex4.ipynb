{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeiraLIyb9gUPlaTgSvPgo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RazvanGolan/Faculty-3rd-year/blob/main/pi/test2/pi_test2_ex4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub5ipbiN5_WB"
      },
      "outputs": [],
      "source": [
        "# exercise 4\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display\n",
        "import warnings\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import tarfile\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.normal(0.0, 1, (1000, 2))\n",
        "A = torch.tensor([[1, 2], [-0.1, 0.5]])\n",
        "b = torch.tensor([1, 2])\n",
        "data = torch.matmul(X, A) + b"
      ],
      "metadata": {
        "id": "mI8tKfNl6VqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_figsize(figsize=(3.5, 2.5)):\n",
        "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
        "    plt.rcParams['figure.figsize'] = figsize"
      ],
      "metadata": {
        "id": "olJ_oocx6Gmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((24, 24)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(0.5, 0.5)\n",
        "])"
      ],
      "metadata": {
        "id": "lRDl2LDX6Hx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.FashionMNIST(\n",
        "    root = \"data\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms\n",
        ")"
      ],
      "metadata": {
        "id": "RLKlg-EK79uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "data_iter = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle = True)"
      ],
      "metadata": {
        "id": "xqYciEMn6I4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_G = nn.Sequential(nn.Linear(2, 2))"
      ],
      "metadata": {
        "id": "NWKSYLY86Jlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class D_block(nn.Module):\n",
        "    def __init__(self, out_channels, in_channels=1, **kwargs):\n",
        "        super(D_block, self).__init__(**kwargs)\n",
        "\n",
        "        self.first_conv = nn.Conv2d(1, out_channels, kernel_size=1)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "        self.last = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
        "        self.batch = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.left = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1)\n",
        "        self.middle = nn.Conv2d(out_channels, out_channels, kernel_size=2, stride=2)\n",
        "        self.right = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2)\n",
        "\n",
        "    def forward(self, X):\n",
        "        print(X.shape)\n",
        "        first = self.first_conv(X)\n",
        "        first = self.activation(first)\n",
        "\n",
        "        out = torch.split(first, 3)\n",
        "\n",
        "        left = self.left(out)\n",
        "        right = self.right(out)\n",
        "        middle = self.middle(out)\n",
        "        middle = self.activation(middle)\n",
        "\n",
        "        out = torch.cat((left, middle, right), dim=1)\n",
        "\n",
        "        out = self.last(out)\n",
        "        out = self.batch(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "vQ54OxeA8H2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_D = nn.Sequential(\n",
        "    D_block(96),\n",
        "    D_block(96),\n",
        "    D_block(96),\n",
        "    nn.Conv2d(96, 1, kernel_size=1)\n",
        "   )"
      ],
      "metadata": {
        "id": "W06Oqgmp6KbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_D(X, Z, net_D, net_G, loss, trainer_D):\n",
        "    \"\"\"Update discriminator.\"\"\"\n",
        "    batch_size = X.shape[0]\n",
        "    ones = torch.ones((batch_size,), device=X.device)\n",
        "    zeros = torch.zeros((batch_size,), device=X.device)\n",
        "    trainer_D.zero_grad()\n",
        "    real_Y = net_D(X)\n",
        "    fake_X = net_G(Z)\n",
        "    # Do not need to compute gradient for `net_G`, detach it from\n",
        "    # computing gradients.\n",
        "    fake_Y = net_D(fake_X.detach())\n",
        "    loss_D = (loss(real_Y, ones.reshape(real_Y.shape)) +\n",
        "              loss(fake_Y, zeros.reshape(fake_Y.shape))) / 2\n",
        "    loss_D.backward()\n",
        "    trainer_D.step()\n",
        "    return loss_D"
      ],
      "metadata": {
        "id": "sp_JpV8v6LIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_G(Z, net_D, net_G, loss, trainer_G):\n",
        "    \"\"\"Update generator.\"\"\"\n",
        "    batch_size = Z.shape[0]\n",
        "    ones = torch.ones((batch_size,), device=Z.device)\n",
        "    trainer_G.zero_grad()\n",
        "    # We could reuse `fake_X` from `update_D` to save computation\n",
        "    fake_X = net_G(Z)\n",
        "    # Recomputing `fake_Y` is needed since `net_D` is changed\n",
        "    fake_Y = net_D(fake_X)\n",
        "    loss_G = loss(fake_Y, ones.reshape(fake_Y.shape))\n",
        "    loss_G.backward()\n",
        "    trainer_G.step()\n",
        "    return loss_G"
      ],
      "metadata": {
        "id": "vXKj7Gu66L55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
        "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
        "    axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)\n",
        "    axes.set_xscale(xscale), axes.set_yscale(yscale)\n",
        "    axes.set_xlim(xlim),     axes.set_ylim(ylim)\n",
        "    if legend:\n",
        "        axes.legend(legend)\n",
        "    axes.grid()\n",
        "\n",
        "class Animator:\n",
        "    \"\"\"For plotting data in animation.\"\"\"\n",
        "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
        "                 ylim=None, xscale='linear', yscale='linear',\n",
        "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
        "                 figsize=(3.5, 2.5)):\n",
        "        \"\"\"Defined in :numref:`sec_utils`\"\"\"\n",
        "        # Incrementally plot multiple lines\n",
        "        if legend is None:\n",
        "            legend = []\n",
        "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "        if nrows * ncols == 1:\n",
        "            self.axes = [self.axes, ]\n",
        "        # Use a lambda function to capture arguments\n",
        "        self.config_axes = lambda: set_axes(\n",
        "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
        "        self.X, self.Y, self.fmts = None, None, fmts\n",
        "\n",
        "    def add(self, x, y):\n",
        "        # Add multiple data points into the figure\n",
        "        if not hasattr(y, \"__len__\"):\n",
        "            y = [y]\n",
        "        n = len(y)\n",
        "        if not hasattr(x, \"__len__\"):\n",
        "            x = [x] * n\n",
        "        if not self.X:\n",
        "            self.X = [[] for _ in range(n)]\n",
        "        if not self.Y:\n",
        "            self.Y = [[] for _ in range(n)]\n",
        "        for i, (a, b) in enumerate(zip(x, y)):\n",
        "            if a is not None and b is not None:\n",
        "                self.X[i].append(a)\n",
        "                self.Y[i].append(b)\n",
        "        self.axes[0].cla()\n",
        "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
        "            self.axes[0].plot(x, y, fmt)\n",
        "        self.config_axes()\n",
        "        display.display(self.fig)\n",
        "        display.clear_output(wait=True)"
      ],
      "metadata": {
        "id": "I-jKxP2J6Mom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):\n",
        "    loss = nn.BCEWithLogitsLoss(reduction='sum')\n",
        "    for w in net_D.parameters():\n",
        "        nn.init.normal_(w, 0, 0.02)\n",
        "    for w in net_G.parameters():\n",
        "        nn.init.normal_(w, 0, 0.02)\n",
        "    trainer_D = torch.optim.Adam(net_D.parameters(), lr=lr_D)\n",
        "    trainer_G = torch.optim.Adam(net_G.parameters(), lr=lr_G)\n",
        "    animator = Animator(xlabel='epoch', ylabel='loss',\n",
        "                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),\n",
        "                            legend=['discriminator', 'generator'])\n",
        "    animator.fig.subplots_adjust(hspace=0.3)\n",
        "    D_loss = 0\n",
        "    G_loss = 0\n",
        "    total_examples = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train one epoch\n",
        "        for X, _ in data_iter:\n",
        "            batch_size = X.shape[0]\n",
        "            Z = torch.normal(0, 1, size=(batch_size, latent_dim))\n",
        "            D_loss += update_D(X, Z, net_D, net_G, loss, trainer_D)\n",
        "            G_loss += update_G(Z, net_D, net_G, loss, trainer_G)\n",
        "            total_examples += batch_size\n",
        "        # Visualize generated examples\n",
        "        Z = torch.normal(0, 1, size=(100, latent_dim))\n",
        "        fake_X = net_G(Z).detach().numpy()\n",
        "        animator.axes[1].cla()\n",
        "        animator.axes[1].scatter(data[:, 0], data[:, 1])\n",
        "        animator.axes[1].scatter(fake_X[:, 0], fake_X[:, 1])\n",
        "        animator.axes[1].legend(['real', 'generated'])\n",
        "        # Show the losses\n",
        "        loss_D, loss_G = D_loss/total_examples, G_loss/total_examples\n",
        "        animator.add(epoch + 1, (loss_D.cpu().detach().numpy(), loss_G.cpu().detach().numpy()))\n",
        "    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, ')"
      ],
      "metadata": {
        "id": "BeRt8Kbn6NkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_D, lr_G, latent_dim, num_epochs = 0.05, 0.05, 90, 1\n",
        "train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G,\n",
        "      latent_dim, data[:100].detach().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        },
        "id": "cbDOvEe36OTX",
        "outputId": "af8244a7-404c-4f0f-9e73-93c510003825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 24, 24])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "conv2d() received an invalid combination of arguments - got (tuple, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!tuple of (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!tuple of (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Ten",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-f6cb67c80f2c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G,\n\u001b[0m\u001b[1;32m      3\u001b[0m       latent_dim, data[:100].detach().numpy())\n",
            "\u001b[0;32m<ipython-input-77-364560d3ed99>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mD_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mupdate_D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer_D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mG_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mupdate_G\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer_G\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mtotal_examples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-90a34e6925c0>\u001b[0m in \u001b[0;36mupdate_D\u001b[0;34m(X, Z, net_D, net_G, loss, trainer_D)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mzeros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrainer_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mreal_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mfake_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_G\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Do not need to compute gradient for `net_G`, detach it from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-619cf12aa6ce>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmiddle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiddle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (tuple, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!tuple of (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!tuple of (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Ten"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rbYx2ioY-wyK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}